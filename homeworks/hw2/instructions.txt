Lab due Feb 25, 2024 23:59 CST
In this programming assignment, you will implement several algorithms introduced in Chapter 4, 5, 6, and 7. General instructions and the requirements can be seen in the attached skeleton code. Please do not change the function interface. In order to know how the functions will be called for grading, please checkout the provided test.py and other class interfaces defined in env.py and policy.py.

First, you need to implement two dynamic programming algorithms: value prediction (Iterative Policy Evaluation on Page 75 of the textbook) and value iteration (page 83). Open dp.py, insert your code, and upload the file below.

Second, you need to implement an off-policy Monte-Carlo evaluation method. You need to write two different versions: (1) ordinary importance sampling and (2) weighted importance sampling (page 110 shows weighted evaluation, you need to adapt that to be unweighted as described on 109). Both are done in an every-visit manner. Open monte_carlo.py, insert your code, and upload the file. Again, please do not change the function interface. Your code should work without modifying any of test.py. Go through policy.py to understand the interface for the policy objects you will need to work with. It is also a good idea to go through env.py to understand how to get information about state and action spaces, as well as transition dynamics, rewards and discounting.

Finally, implement (1) On-policy n-step TD for evaluating a policy and (2) off-policy n-step SARSA for learning an optimal policy (page 144 and 149 resp.). Open n_step_bootstrap.py, insert your code, and upload the file. Note: There is an error in the 2018 edition of the textbook, which is corrected in the 2020 edition. Please see the authors' errata and notes for page 149.

test.py includes some rudimentary testing code including a very simple MDP. Your code will be tested on different environments than the provided one, so please implement a more difficult MDP (by implementing an environment yourself), and test your algorithm thoroughly before you submit. The goal of this assignment is getting a deeper understanding of RL with experience of your own.  It is very important to implement algorithms very carefully since these algorithms can fail due to minor implementation errors, which are almost impossible to debug. That said, although it is generally recommended to implement the algorithms exactly as given in the textbook, we encourage you to attempt to make improvements to the efficiency or more elegant implementations than the book.

We especially encourage you to develop environments that contain interesting characteristics, such as infinite variance as shown in Figure 5.4 of the textbook. Implementing environments and reproducing figures in the textbook will be good practice before submission. For this purpose, we recommend using the OpenAI Gym interface. Even though that is not used in this assignment, exposing your environment as a Gym environment will help you learn the interface, and will also prove useful for subsequent assignments.

Note: When using test.py for testing your code, you might need to comment out some of the tests if you haven't implemented the respective portions yet.

For testing your code and minimizing the chance of disparity in runtime and results with the grader, it is recommended to create a fresh Anaconda environment with Python 3.7 and then install numpy with pip (not conda). You might also need to install tqdm to make sure test.py works.